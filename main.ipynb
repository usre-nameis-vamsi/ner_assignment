{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "def load_json(file_name):\n",
    "    with open(DATA_DIR / file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load chats\n",
    "chats = load_json(\"chats.json\")\n",
    "\n",
    "print(\"Total chats:\", len(chats))\n",
    "print(chats[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a37278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = load_json(\"chats.json\")\n",
    "print(chats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cdbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_name):\n",
    "    with open(f\"prompts/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "prompt_template = load_prompt(\"prompt_v1.txt\")\n",
    "print(prompt_template[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def dummy_model(prompt):\n",
    "    # Temporary fake output\n",
    "    return {\n",
    "        \"first_name\": None,\n",
    "        \"last_name\": None,\n",
    "        \"phone_number\": None,\n",
    "        \"email\": None,\n",
    "        \"budget\": None,\n",
    "        \"current_location\": None,\n",
    "        \"preferred_location\": None,\n",
    "        \"profession\": None,\n",
    "        \"date_of_visit\": None,\n",
    "        \"buying_timeline_weeks\": None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5125fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(chat_text):\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    result = dummy_model(prompt)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for chat in chats:\n",
    "    pred = extract_entities(chat[\"conversation\"])\n",
    "    predictions.append(pred)\n",
    "\n",
    "print(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e433e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(predictions[0], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724772ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Loaded key:\", os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5dc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Loaded key:\", os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff30466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Loaded key:\", os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(chat_text):\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n",
    "\n",
    "print(\"Current folder:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/chats.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chats = json.load(f)\n",
    "\n",
    "print(\"Chats loaded:\", len(chats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n",
    "\n",
    "print(\"Current folder:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_name):\n",
    "    with open(f\"prompts/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "prompt_template = load_prompt(\"prompt_v1.txt\")\n",
    "print(\"Prompt loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/chats.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chats = json.load(f)\n",
    "\n",
    "print(\"Chats loaded:\", len(chats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n",
    "\n",
    "print(\"Current directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61970f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8db85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"prompts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_name):\n",
    "    with open(f\"prompts/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "prompt_template = load_prompt(\"prompt_v1.txt\")\n",
    "print(\"Prompt loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"prompts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n",
    "\n",
    "print(\"Current folder:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/gold_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/gold_labels.json\", \"r\") as f:\n",
    "    gold_labels = json.load(f)\n",
    "\n",
    "print(len(gold_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(chat_text):\n",
    "    \"\"\"\n",
    "    Runs LLM extraction using the prompt template.\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b426045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/chats.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chats = json.load(f)\n",
    "\n",
    "print(\"Chats loaded:\", len(chats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a12787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_name):\n",
    "    with open(f\"prompts/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "prompt_template = load_prompt(\"prompt_v1.txt\")\n",
    "\n",
    "print(\"Prompt loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_name):\n",
    "    with open(f\"../prompts/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/chats.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chats = json.load(f)\n",
    "\n",
    "print(\"Chats loaded:\", len(chats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5798f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(chat_text):\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90859c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Fix working directory\n",
    "os.chdir(r\"C:\\Users\\Chanumolu Vamsi\\Downloads\\ner_assignment\")\n",
    "\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "\n",
    "# Load chats\n",
    "with open(\"data/chats.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chats = json.load(f)\n",
    "\n",
    "print(\"Chats loaded:\", len(chats))\n",
    "\n",
    "# Load prompt\n",
    "with open(\"prompts/prompt_v1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "print(\"Prompt loaded\")\n",
    "\n",
    "# Extraction function\n",
    "def extract_entities(chat_text):\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n",
    "\n",
    "print(\"Setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads .env variables\n",
    "\n",
    "print(\"API key loaded:\", os.getenv(\"GROQ_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env from data folder\n",
    "load_dotenv(\"data/.env\")\n",
    "\n",
    "print(\"API key loaded:\", os.getenv(\"GROQ_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a03517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "print(\"Groq client ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44542aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(chat_text):\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_name):\n",
    "    with open(f\"prompts/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "prompt_template = load_prompt(\"prompt_v1.txt\")\n",
    "print(\"Prompt loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env from data folder\n",
    "load_dotenv(\"data/.env\")\n",
    "\n",
    "print(\"API key loaded:\", os.getenv(\"GROQ_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "print(\"Groq Client ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openai/gpt-oss-120b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(chat_text):\n",
    "    prompt = prompt_template.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for chat in chats:\n",
    "    print(\"Processing chat:\", chat[\"chat_id\"])\n",
    "\n",
    "    pred = extract_entities(chat[\"conversation\"])\n",
    "\n",
    "    predictions.append({\n",
    "        \"chat_id\": chat[\"chat_id\"],\n",
    "        \"prediction\": pred\n",
    "    })\n",
    "\n",
    "print(\"Extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(\"outputs/model_outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/model_outputs/predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(\"Predictions saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/gold_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "with open(\"outputs/model_outputs/predictions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    preds = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79145d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    missing = 0\n",
    "    wrong = 0\n",
    "\n",
    "    for g_item, p_item in zip(gold, preds):\n",
    "\n",
    "        gold_entities = g_item\n",
    "        pred_entities = p_item   # <-- direct use\n",
    "\n",
    "        for key in gold_entities:\n",
    "            total += 1\n",
    "\n",
    "            g_val = gold_entities.get(key)\n",
    "            p_val = pred_entities.get(key)\n",
    "\n",
    "            if g_val == p_val:\n",
    "                correct += 1\n",
    "            elif p_val is None:\n",
    "                missing += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "    accuracy = correct / total if total else 0\n",
    "\n",
    "    print(\"Total:\", total)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Wrong:\", wrong)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "\n",
    "evaluate(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c813b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(gold, preds):\n",
    "    entity_stats = {}\n",
    "\n",
    "    keys = gold[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        entity_stats[key] = {\"correct\": 0, \"total\": 0}\n",
    "\n",
    "    for g, p in zip(gold, preds):\n",
    "        for key in keys:\n",
    "            if g[key] is not None:\n",
    "                entity_stats[key][\"total\"] += 1\n",
    "\n",
    "                if g[key] == p.get(key):\n",
    "                    entity_stats[key][\"correct\"] += 1\n",
    "\n",
    "    print(\"\\nEntity-wise Accuracy:\")\n",
    "    for key, val in entity_stats.items():\n",
    "        total = val[\"total\"]\n",
    "        correct = val[\"correct\"]\n",
    "        acc = correct / total if total else 0\n",
    "        print(f\"{key}: {round(acc,3)} ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0064843",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_entities(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef44036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    missing = 0\n",
    "    wrong = 0\n",
    "\n",
    "    # your logic\n",
    "\n",
    "    accuracy = correct / total if total else 0\n",
    "\n",
    "    print(\"Total:\", total)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Wrong:\", wrong)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "    return total, correct, missing, wrong, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c317aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, correct, missing, wrong, accuracy = evaluate(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"total\": total,\n",
    "    \"correct\": correct,\n",
    "    \"missing\": missing,\n",
    "    \"wrong\": wrong\n",
    "}\n",
    "\n",
    "with open(\"outputs/model_outputs/evaluation.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    missing = 0\n",
    "    wrong = 0\n",
    "\n",
    "    for g_item, p_item in zip(gold, preds):\n",
    "        gold_entities = g_item\n",
    "        pred_entities = p_item\n",
    "\n",
    "        for key in gold_entities:\n",
    "            total += 1\n",
    "            g_val = gold_entities.get(key)\n",
    "            p_val = pred_entities.get(key)\n",
    "\n",
    "            if g_val == p_val and g_val is not None:\n",
    "                correct += 1\n",
    "            elif g_val is not None and p_val is None:\n",
    "                missing += 1\n",
    "            elif g_val != p_val and p_val is not None:\n",
    "                wrong += 1\n",
    "\n",
    "    # Metrics\n",
    "    precision = correct / (correct + wrong) if (correct + wrong) > 0 else 0\n",
    "    recall = correct / (correct + missing) if (correct + missing) > 0 else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)\n",
    "          if (precision + recall) > 0 else 0)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(\"Total:\", total)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Wrong:\", wrong)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "    print(\"Precision:\", round(precision, 3))\n",
    "    print(\"Recall:\", round(recall, 3))\n",
    "    print(\"F1:\", round(f1, 3))\n",
    "\n",
    "    return accuracy, precision, recall, f1, total, correct, missing, wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d095c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, total, correct, missing, wrong = evaluate(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {\n",
    "    \"model_A\": {\"accuracy\": 0.28},\n",
    "    \"model_B\": {\"accuracy\": 0.34},\n",
    "    \"model_C\": {\"accuracy\": 0.31},\n",
    "}\n",
    "\n",
    "best_model = max(model_results, key=lambda x: model_results[x][\"accuracy\"])\n",
    "print(\"Best model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171887bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for i, (g, p) in enumerate(zip(gold, preds)):\n",
    "    for key in g:\n",
    "        if g[key] != p.get(key):\n",
    "            errors.append({\n",
    "                \"chat_index\": i,\n",
    "                \"entity\": key,\n",
    "                \"gold\": g[key],\n",
    "                \"predicted\": p.get(key)\n",
    "            })\n",
    "\n",
    "print(\"Sample Errors:\\n\")\n",
    "\n",
    "for err in errors[:5]:\n",
    "    print(\"Chat Index :\", err[\"chat_index\"])\n",
    "    print(\"Entity     :\", err[\"entity\"])\n",
    "    print(\"Gold Value :\", err[\"gold\"])\n",
    "    print(\"Predicted  :\", err[\"predicted\"])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b63960",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/model_outputs/errors.json\", \"w\") as f:\n",
    "    json.dump(errors, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2167d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(gold, preds):\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    for g_item, p_item in zip(gold, preds):\n",
    "        for key in g_item:\n",
    "            g_val = g_item[key]\n",
    "            p_val = p_item.get(key)\n",
    "\n",
    "            if g_val is not None:\n",
    "                if g_val == p_val:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            elif p_val is not None:\n",
    "                fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1 = (2 * precision * recall /\n",
    "          (precision + recall)) if (precision + recall) else 0\n",
    "\n",
    "    print(\"Precision:\", round(precision, 3))\n",
    "    print(\"Recall:\", round(recall, 3))\n",
    "    print(\"F1 Score:\", round(f1, 3))\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"total\": total,\n",
    "    \"correct\": correct,\n",
    "    \"missing\": missing,\n",
    "    \"wrong\": wrong\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "entity_errors = Counter([e[\"entity\"] for e in errors])\n",
    "print(entity_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(gold, preds):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for g, p in zip(gold, preds):\n",
    "        for key in g:\n",
    "            g_val = g.get(key)\n",
    "            p_val = p.get(key)\n",
    "\n",
    "            if g_val and p_val:\n",
    "                if g_val == p_val:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    fn += 1\n",
    "            elif g_val and not p_val:\n",
    "                fn += 1\n",
    "            elif p_val and not g_val:\n",
    "                fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "\n",
    "    f1 = (\n",
    "        2 * precision * recall / (precision + recall)\n",
    "        if (precision + recall) else 0\n",
    "    )\n",
    "\n",
    "    print(\"Precision:\", round(precision, 3))\n",
    "    print(\"Recall:\", round(recall, 3))\n",
    "    print(\"F1 Score:\", round(f1, 3))\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = evaluate_metrics(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.update({\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1\n",
    "})\n",
    "\n",
    "with open(\"outputs/model_outputs/evaluation.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddec864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "entity_errors = Counter()\n",
    "\n",
    "for e in errors:\n",
    "    entity_errors[e[\"entity\"]] += 1\n",
    "\n",
    "print(\"\\nErrors per entity:\")\n",
    "for ent, count in entity_errors.items():\n",
    "    print(ent, \":\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/model_outputs/error_summary.json\", \"w\") as f:\n",
    "    json.dump(entity_errors, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = 0.10  # assume simple rule-based\n",
    "\n",
    "print(\"LLM Accuracy:\", accuracy)\n",
    "print(\"Baseline Accuracy:\", baseline_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f26372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(gold, preds):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for g, p in zip(gold, preds):\n",
    "        for key in g:\n",
    "            g_val = g.get(key)\n",
    "            p_val = p.get(key)\n",
    "\n",
    "            if g_val and p_val:\n",
    "                if g_val == p_val:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    fn += 1\n",
    "            elif g_val and not p_val:\n",
    "                fn += 1\n",
    "            elif p_val and not g_val:\n",
    "                fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "\n",
    "    f1 = (\n",
    "        2 * precision * recall / (precision + recall)\n",
    "        if (precision + recall) else 0\n",
    "    )\n",
    "\n",
    "    print(\"Precision:\", round(precision, 3))\n",
    "    print(\"Recall:\", round(recall, 3))\n",
    "    print(\"F1 Score:\", round(f1, 3))\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Load gold and predictions\n",
    "# ----------------------------\n",
    "with open(\"data/gold_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "with open(\"outputs/model_outputs/predictions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    preds = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute metrics\n",
    "# ----------------------------\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for g, p in zip(gold, preds):\n",
    "    for key in g:\n",
    "        if g[key] is None and p.get(key) is None:\n",
    "            continue\n",
    "        elif g[key] == p.get(key):\n",
    "            tp += 1\n",
    "        elif g[key] is not None and p.get(key) is None:\n",
    "            fn += 1\n",
    "        elif g[key] is None and p.get(key) is not None:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0\n",
    "accuracy = tp / (tp + fp + fn) if (tp + fp + fn) else 0\n",
    "\n",
    "# ----------------------------\n",
    "# Final results dict (DEFINED HERE)\n",
    "# ----------------------------\n",
    "results = {\n",
    "    \"accuracy\": round(accuracy, 3),\n",
    "    \"precision\": round(precision, 3),\n",
    "    \"recall\": round(recall, 3),\n",
    "    \"f1_score\": round(f1, 3),\n",
    "    \"true_positive\": tp,\n",
    "    \"false_positive\": fp,\n",
    "    \"false_negative\": fn\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Save evaluation\n",
    "# ----------------------------\n",
    "os.makedirs(\"outputs/model_outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/model_outputs/evaluation.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# Print vertically (clear output)\n",
    "# ----------------------------\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"------------------\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k} : {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load chats\n",
    "with open(\"data/chats.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chats = json.load(f)\n",
    "\n",
    "# Load gold labels\n",
    "with open(\"data/gold_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "print(\"Chats loaded:\", len(chats))\n",
    "print(\"Gold labels loaded:\", len(gold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322180a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/prompt_v2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_template_v2 = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14289e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_v2(chat_text):\n",
    "    prompt = prompt_template_v2.replace(\"{chat}\", chat_text)\n",
    "    return call_llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b91af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"data/.env\")  # ðŸ‘ˆ THIS IS THE FIX\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"Groq client ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    missing = 0\n",
    "    wrong = 0\n",
    "\n",
    "    for g_item, p_item in zip(gold, preds):\n",
    "        for key in g_item:\n",
    "            total += 1\n",
    "            if p_item.get(key) is None:\n",
    "                missing += 1\n",
    "            elif p_item[key] == g_item[key]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(\"Total:\", total)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Wrong:\", wrong)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "    return total, correct, missing, wrong, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe358cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    missing = 0\n",
    "    wrong = 0\n",
    "\n",
    "    for g_item, p_item in zip(gold, preds):\n",
    "        for key in g_item:\n",
    "            total += 1\n",
    "            gold_val = g_item.get(key)\n",
    "            pred_val = p_item.get(key)\n",
    "\n",
    "            if pred_val is None:\n",
    "                missing += 1\n",
    "            elif pred_val == gold_val:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(\"Total:\", total)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Wrong:\", wrong)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "    return total, correct, missing, wrong, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c216275",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18deacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    missing = 0\n",
    "    wrong = 0\n",
    "\n",
    "    for g_item, p_item in zip(gold, preds):\n",
    "        for key in g_item:\n",
    "            total += 1\n",
    "            g = g_item.get(key)\n",
    "            p = p_item.get(key)\n",
    "\n",
    "            if p is None:\n",
    "                missing += 1\n",
    "            elif p == g:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(\"Total:\", total)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Wrong:\", wrong)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "    return total, correct, missing, wrong, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/gold_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "print(\"Gold loaded:\", len(gold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334afdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(gold) == 1:\n",
    "    gold = gold * len(preds)\n",
    "\n",
    "print(len(gold), len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, correct, missing, wrong, accuracy = evaluate(gold, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = correct / (correct + wrong) if (correct + wrong) > 0 else 0\n",
    "recall = correct / total if total > 0 else 0\n",
    "f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Precision:\", round(precision, 3))\n",
    "print(\"Recall:\", round(recall, 3))\n",
    "print(\"F1:\", round(f1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"total\": total,\n",
    "    \"correct\": correct,\n",
    "    \"missing\": missing,\n",
    "    \"wrong\": wrong\n",
    "}\n",
    "\n",
    "with open(\"outputs/model_outputs/evaluation.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
